{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tWQnVjqfFwv"
   },
   "source": [
    "## Data Leakage, L1(Lasso) and L2 (Ridge) regularization using Linear Regression\n",
    "\n",
    "We will use cross validation, lasso and ridge regression in this lab.\n",
    "\n",
    "Specifically speaking, <br>\n",
    "Regularization basically adds the penalty as model complexity increases.<br>\n",
    "Cross validation is used to evaluate how well our model can generalize on the dataset. <br>\n",
    "\n",
    "We will be using r2 score in this lab. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model.\n",
    "\n",
    "\n",
    "In this task, we will explore the following things on linear regression model:\n",
    "- Cross Validation\n",
    "- L1 regularization (Lasso regression)\n",
    "- L2 regularization (Ridge regression)\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "The dataset is available at \"data/bike.csv\" in the respective challenge's repo.<br>\n",
    "\n",
    "The dataset is __modified version__ of the dataset 'bike.csv' provided by UCI Machine Learning repository.\n",
    "\n",
    "Original dataset: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "#### Objective\n",
    "To learn about how cross validation, L1 regularization and L2 regularization work.\n",
    "\n",
    "#### Tasks\n",
    "- load the dataset.\n",
    "- perform pre-processing on the data.\n",
    "- remove registered feature and keep the casual feature to understand data leakage.\n",
    "- construct train and test dataset.\n",
    "- create a linear regression model.\n",
    "- check the r2 score of the initial linear regression model on train and test dataset\n",
    "- observe distribution of weights in the initial linear regression model. \n",
    "- split the dataset into k consecutive folds.\n",
    "- calculate cross validation score for the k fold and check how well our model can generalize on the training dataset.\n",
    "- checking the variance threshold of dataset and remove features with low variance.\n",
    "- apply L1 regularization on the dataset and check the r2_score.\n",
    "- visualize the distribution of weights on the lasso regression model.\n",
    "- apply L2 regularization on the dataset and check the r2_score.\n",
    "- visualize the distribution of weights on the ridge regression model. \n",
    "\n",
    "#### Further fun\n",
    "- apply RFE on the dataset to automatically remove uneccessary features which would prevent overfitting.\n",
    "- don't remove casual and registered features and check the effect of data leakage on the model\n",
    "- implement lasso and ridge regression without using inbuilt librarires.\n",
    "- apply elastic net to visualize the effect of both ridge and lasso regression.\n",
    "\n",
    "\n",
    "#### Helpful links\n",
    "- Cross validation : https://machinelearningmastery.com/k-fold-cross-validation/#:~:text=Cross%2Dvalidation%20is%20a%20resampling,k%2Dfold%20cross%2Dvalidation.\n",
    "- Cross validation: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "- L1 and L2 regularization : https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "- L1 and L2 regularization : https://www.youtube.com/watch?v=9lRv01HDU0s&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=30&t=904s\n",
    "- r2_score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score\n",
    "- pd.get_dummies() and One Hot Encoding: https://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
    "- Data Leakage : \"https://machinelearningmastery.com/data-leakage-machine-learning/\n",
    "- sklearn k-fold : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "- sklearn cross_val_score : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score\n",
    "- sklearn lasso regression : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso\n",
    "- sklearn ridge regression : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge\n",
    "- RFE : https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "- RFE sklearn : https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "- Use slack for doubts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRl7KPgwjRHI"
   },
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Sklearn linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sklearn regression model evaluation functions\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Perform feature selection using a variance threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Feature selection using Recursive Feature Elimimation\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq1jiQz-kIpA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>feel_temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>promotion_level</th>\n",
       "      <th>promotion_type</th>\n",
       "      <th>promotion_level_external</th>\n",
       "      <th>promotion_type_external</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12765</td>\n",
       "      <td>winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>cloud</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>226</td>\n",
       "      <td>654</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12766</td>\n",
       "      <td>winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cloud</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>670</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12767</td>\n",
       "      <td>winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>1229</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12768</td>\n",
       "      <td>winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>1454</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12769</td>\n",
       "      <td>winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1518</td>\n",
       "      <td>1576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  season  year  holiday  weekday  workingday weather      temp  \\\n",
       "0  12765  winter  2016        0        6           0   cloud  0.344167   \n",
       "1  12766  winter  2016        0        0           0   cloud  0.363478   \n",
       "2  12767  winter  2016        0        1           1    fair  0.196364   \n",
       "3  12768  winter  2016        0        2           1    fair  0.200000   \n",
       "4  12769  winter  2016        0        3           1    fair  0.226957   \n",
       "\n",
       "   feel_temp       hum  windspeed  promotion_level  promotion_type  \\\n",
       "0   0.363625  0.805833   0.160446                7               1   \n",
       "1   0.353739  0.696087   0.248539                8               1   \n",
       "2   0.189405  0.437273   0.248309                3               1   \n",
       "3   0.212122  0.590435   0.160296                0               1   \n",
       "4   0.229270  0.436957   0.186900                2               0   \n",
       "\n",
       "   promotion_level_external  promotion_type_external  casual  registered   cnt  \n",
       "0                         2                        2     226         654   880  \n",
       "1                         8                        1     125         670   795  \n",
       "2                        10                        2      75        1229  1304  \n",
       "3                         8                        3      67        1454  1521  \n",
       "4                         5                        1      58        1518  1576  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data and inspect the first 5 rows\n",
    "data = pd.read_csv('data/bike.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw5T1NvekL65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'season', 'year', 'holiday', 'weekday', 'workingday', 'weather',\n",
      "       'temp', 'feel_temp', 'hum', 'windspeed', 'promotion_level',\n",
      "       'promotion_type', 'promotion_level_external', 'promotion_type_external',\n",
      "       'casual', 'registered', 'cnt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print the data types of each feature name\n",
    "a=data.columns\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RE0sjCi4kREL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 365 entries, 0 to 364\n",
      "Data columns (total 18 columns):\n",
      "id                          365 non-null int64\n",
      "season                      365 non-null object\n",
      "year                        365 non-null int64\n",
      "holiday                     365 non-null int64\n",
      "weekday                     365 non-null int64\n",
      "workingday                  365 non-null int64\n",
      "weather                     365 non-null object\n",
      "temp                        365 non-null float64\n",
      "feel_temp                   365 non-null float64\n",
      "hum                         365 non-null float64\n",
      "windspeed                   365 non-null float64\n",
      "promotion_level             365 non-null int64\n",
      "promotion_type              365 non-null int64\n",
      "promotion_level_external    365 non-null int64\n",
      "promotion_type_external     365 non-null int64\n",
      "casual                      365 non-null int64\n",
      "registered                  365 non-null int64\n",
      "cnt                         365 non-null int64\n",
      "dtypes: float64(4), int64(12), object(2)\n",
      "memory usage: 51.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# check for null values in each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWJz8TAgketK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12765 12766 12767 12768 12769 12770 12771 12772 12773 12774 12775 12776\n",
      " 12777 12778 12779 12780 12781 12782 12783 12784 12785 12786 12787 12788\n",
      " 12789 12790 12791 12792 12793 12794 12795 12796 12797 12798 12799 12800\n",
      " 12801 12802 12803 12804 12805 12806 12807 12808 12809 12810 12811 12812\n",
      " 12813 12814 12815 12816 12817 12818 12819 12820 12821 12822 12823 12824\n",
      " 12825 12826 12827 12828 12829 12830 12831 12832 12833 12834 12835 12836\n",
      " 12837 12838 12839 12840 12841 12842 12843 12844 12845 12846 12847 12848\n",
      " 12849 12850 12851 12852 12853 12854 12855 12856 12857 12858 12859 12860\n",
      " 12861 12862 12863 12864 12865 12866 12867 12868 12869 12870 12871 12872\n",
      " 12873 12874 12875 12876 12877 12878 12879 12880 12881 12882 12883 12884\n",
      " 12885 12886 12887 12888 12889 12890 12891 12892 12893 12894 12895 12896\n",
      " 12897 12898 12899 12900 12901 12902 12903 12904 12905 12906 12907 12908\n",
      " 12909 12910 12911 12912 12913 12914 12915 12916 12917 12918 12919 12920\n",
      " 12921 12922 12923 12924 12925 12926 12927 12928 12929 12930 12931 12932\n",
      " 12933 12934 12935 12936 12937 12938 12939 12940 12941 12942 12943 12944\n",
      " 12945 12946 12947 12948 12949 12950 12951 12952 12953 12954 12955 12956\n",
      " 12957 12958 12959 12960 12961 12962 12963 12964 12965 12966 12967 12968\n",
      " 12969 12970 12971 12972 12973 12974 12975 12976 12977 12978 12979 12980\n",
      " 12981 12982 12983 12984 12985 12986 12987 12988 12989 12990 12991 12992\n",
      " 12993 12994 12995 12996 12997 12998 12999 13000 13001 13002 13003 13004\n",
      " 13005 13006 13007 13008 13009 13010 13011 13012 13013 13014 13015 13016\n",
      " 13017 13018 13019 13020 13021 13022 13023 13024 13025 13026 13027 13028\n",
      " 13029 13030 13031 13032 13033 13034 13035 13036 13037 13038 13039 13040\n",
      " 13041 13042 13043 13044 13045 13046 13047 13048 13049 13050 13051 13052\n",
      " 13053 13054 13055 13056 13057 13058 13059 13060 13061 13062 13063 13064\n",
      " 13065 13066 13067 13068 13069 13070 13071 13072 13073 13074 13075 13076\n",
      " 13077 13078 13079 13080 13081 13082 13083 13084 13085 13086 13087 13088\n",
      " 13089 13090 13091 13092 13093 13094 13095 13096 13097 13098 13099 13100\n",
      " 13101 13102 13103 13104 13105 13106 13107 13108 13109 13110 13111 13112\n",
      " 13113 13114 13115 13116 13117 13118 13119 13120 13121 13122 13123 13124\n",
      " 13125 13126 13127 13128 13129]\n",
      "['winter' 'spring' 'summer' 'autumn']\n",
      "[2016]\n",
      "[0 1]\n",
      "[6 0 1 2 3 4 5]\n",
      "[0 1]\n",
      "['cloud' 'fair' 'rain']\n",
      "[0.344167  0.363478  0.196364  0.2       0.226957  0.204348  0.196522\n",
      " 0.165     0.138333  0.150833  0.169091  0.172727  0.16087   0.233333\n",
      " 0.231667  0.175833  0.216667  0.292174  0.261667  0.1775    0.0591304\n",
      " 0.0965217 0.0973913 0.223478  0.2175    0.195     0.203478  0.216522\n",
      " 0.180833  0.192174  0.26      0.186957  0.211304  0.285833  0.271667\n",
      " 0.220833  0.134783  0.144348  0.189091  0.2225    0.316522  0.415\n",
      " 0.266087  0.318261  0.435833  0.521667  0.399167  0.285217  0.303333\n",
      " 0.182222  0.221739  0.295652  0.364348  0.2825    0.343478  0.407273\n",
      " 0.266667  0.335     0.198333  0.384167  0.376522  0.261739  0.2925\n",
      " 0.295833  0.389091  0.329167  0.384348  0.325217  0.317391  0.365217\n",
      " 0.54      0.4725    0.3325    0.430435  0.441667  0.346957  0.285\n",
      " 0.264167  0.265833  0.253043  0.264348  0.3025    0.3       0.268333\n",
      " 0.315     0.378333  0.573333  0.414167  0.390833  0.4375    0.335833\n",
      " 0.3425    0.426667  0.595652  0.5025    0.4125    0.4675    0.446667\n",
      " 0.430833  0.456667  0.5125    0.505833  0.595     0.459167  0.336667\n",
      " 0.46      0.581667  0.606667  0.631667  0.62      0.6175    0.51\n",
      " 0.451667  0.549167  0.616667  0.479167  0.52      0.528333  0.5325\n",
      " 0.5425    0.535     0.520833  0.5625    0.5775    0.561667  0.55\n",
      " 0.530833  0.536667  0.6025    0.604167  0.66      0.660833  0.708333\n",
      " 0.681667  0.655833  0.6675    0.733333  0.775     0.764167  0.715\n",
      " 0.635     0.648333  0.678333  0.7075    0.775833  0.808333  0.755\n",
      " 0.725     0.6925    0.626667  0.628333  0.649167  0.696667  0.699167\n",
      " 0.680833  0.728333  0.724167  0.695     0.68      0.6825    0.744167\n",
      " 0.7225    0.738333  0.716667  0.726667  0.746667  0.72      0.75\n",
      " 0.709167  0.7475    0.7625    0.794167  0.663333  0.686667  0.719167\n",
      " 0.776667  0.768333  0.815     0.848333  0.849167  0.83      0.743333\n",
      " 0.771667  0.779167  0.838333  0.804167  0.805833  0.783333  0.731667\n",
      " 0.71      0.710833  0.7425    0.765     0.766667  0.7175    0.685833\n",
      " 0.676667  0.665833  0.700833  0.723333  0.711667  0.685     0.6975\n",
      " 0.691667  0.640833  0.673333  0.684167  0.7       0.707059  0.636667\n",
      " 0.639167  0.656667  0.655     0.643333  0.669167  0.599167  0.633913\n",
      " 0.65      0.653333  0.644348  0.650833  0.469167  0.491667  0.5075\n",
      " 0.609167  0.634167  0.564167  0.41      0.356667  0.484167  0.538333\n",
      " 0.494167  0.510833  0.540833  0.570833  0.566667  0.543333  0.589167\n",
      " 0.550833  0.506667  0.511667  0.534167  0.541739  0.475833  0.4275\n",
      " 0.4225    0.421667  0.463333  0.471667  0.47      0.330833  0.254167\n",
      " 0.319167  0.34      0.400833  0.3775    0.408333  0.403333  0.326667\n",
      " 0.348333  0.395     0.4       0.38      0.324167  0.440833  0.53\n",
      " 0.341667  0.274167  0.4475    0.416667  0.373333  0.375     0.375833\n",
      " 0.503478  0.458333  0.325     0.3125    0.314167  0.299167  0.385833\n",
      " 0.4625    0.290833  0.275     0.238333  0.3175    0.258333  0.276667\n",
      " 0.428333  0.423333  0.274783  0.321739  0.29913   0.248333  0.311667 ]\n",
      "[0.363625  0.353739  0.189405  0.212122  0.22927   0.233209  0.208839\n",
      " 0.162254  0.116175  0.150888  0.191464  0.160473  0.150883  0.188413\n",
      " 0.248112  0.234217  0.176771  0.232333  0.298422  0.25505   0.157833\n",
      " 0.0790696 0.0988391 0.11793   0.234526  0.2036    0.2197    0.223317\n",
      " 0.212126  0.250322  0.18625   0.23453   0.254417  0.177878  0.228587\n",
      " 0.243058  0.291671  0.303658  0.198246  0.144283  0.149548  0.213509\n",
      " 0.232954  0.324113  0.39835   0.254274  0.3162    0.428658  0.511983\n",
      " 0.391404  0.27733   0.284075  0.186033  0.245717  0.289191  0.350461\n",
      " 0.282192  0.351109  0.400118  0.263879  0.320071  0.200133  0.255679\n",
      " 0.378779  0.366252  0.238461  0.3024    0.286608  0.385668  0.305\n",
      " 0.32575   0.380091  0.332     0.318178  0.36693   0.410333  0.527009\n",
      " 0.466525  0.409735  0.440642  0.337939  0.270833  0.256312  0.257571\n",
      " 0.250339  0.257574  0.292908  0.29735   0.257575  0.283454  0.315637\n",
      " 0.378767  0.542929  0.387608  0.433696  0.324479  0.341529  0.426737\n",
      " 0.565217  0.493054  0.417283  0.462742  0.441913  0.425492  0.445696\n",
      " 0.503146  0.489258  0.564392  0.453892  0.321954  0.450121  0.551763\n",
      " 0.5745    0.594083  0.575142  0.578929  0.497463  0.464021  0.448204\n",
      " 0.532833  0.582079  0.40465   0.441917  0.474117  0.512621  0.518933\n",
      " 0.525246  0.522721  0.5284    0.523363  0.4943    0.500629  0.536\n",
      " 0.550512  0.538529  0.527158  0.510742  0.529042  0.571975  0.590296\n",
      " 0.604813  0.615542  0.654688  0.637008  0.612379  0.61555   0.671092\n",
      " 0.725383  0.720967  0.643942  0.587133  0.594696  0.616804  0.621858\n",
      " 0.65595   0.727279  0.757579  0.703292  0.678038  0.643325  0.601654\n",
      " 0.591546  0.587754  0.595346  0.600383  0.643954  0.645846  0.637646\n",
      " 0.693829  0.693833  0.656583  0.643313  0.637629  0.637004  0.692558\n",
      " 0.652162  0.667308  0.668575  0.665417  0.696338  0.685633  0.686871\n",
      " 0.670483  0.664158  0.690025  0.729804  0.739275  0.689404  0.635104\n",
      " 0.624371  0.638263  0.669833  0.703925  0.747479  0.74685   0.826371\n",
      " 0.840896  0.804287  0.794829  0.720958  0.696979  0.690667  0.7399\n",
      " 0.785967  0.728537  0.729796  0.707071  0.679937  0.664788  0.656567\n",
      " 0.676154  0.715292  0.703283  0.724121  0.684983  0.651521  0.654042\n",
      " 0.645858  0.624388  0.616167  0.645837  0.666671  0.662258  0.633221\n",
      " 0.648996  0.675525  0.638254  0.606067  0.630692  0.645854  0.659733\n",
      " 0.635556  0.647959  0.607958  0.594704  0.611121  0.614921  0.604808\n",
      " 0.633213  0.665429  0.625646  0.5152    0.544229  0.555361  0.578946\n",
      " 0.607962  0.609229  0.60213   0.603554  0.6269    0.553671  0.461475\n",
      " 0.478512  0.490537  0.529675  0.532217  0.550533  0.554963  0.522125\n",
      " 0.564412  0.572637  0.589042  0.574525  0.575158  0.574512  0.544829\n",
      " 0.412863  0.345317  0.392046  0.472858  0.527138  0.480425  0.504404\n",
      " 0.513242  0.523983  0.542925  0.546096  0.517717  0.551804  0.498725\n",
      " 0.503154  0.510725  0.513848  0.423596  0.422333  0.457067  0.463375\n",
      " 0.472846  0.457046  0.318812  0.227913  0.321329  0.356063  0.397088\n",
      " 0.390133  0.405921  0.403392  0.323854  0.362358  0.400871  0.412246\n",
      " 0.409079  0.373721  0.306817  0.357942  0.43055   0.524612  0.507579\n",
      " 0.451988  0.323221  0.272721  0.324483  0.457058  0.445062  0.421696\n",
      " 0.430537  0.372471  0.380671  0.385087  0.4558    0.490122  0.451375\n",
      " 0.311221  0.305554  0.331433  0.310604  0.3491    0.393925  0.4564\n",
      " 0.400246  0.256938  0.317542  0.266412  0.253154  0.270196  0.301138\n",
      " 0.338362  0.412237  0.359825  0.249371  0.245579  0.280933  0.396454\n",
      " 0.428017  0.426121  0.377513  0.299242  0.279961  0.315535  0.327633\n",
      " 0.279974  0.263892  0.414121 ]\n",
      "[0.805833 0.696087 0.437273 0.590435 0.436957 0.518261 0.498696 0.535833\n",
      " 0.434167 0.482917 0.686364 0.599545 0.470417 0.537826 0.49875  0.48375\n",
      " 0.5375   0.861667 0.741739 0.538333 0.457083 0.4      0.436522 0.491739\n",
      " 0.616957 0.8625   0.6875   0.793043 0.651739 0.722174 0.60375  0.829565\n",
      " 0.775417 0.437826 0.585217 0.929167 0.568333 0.738333 0.537917 0.494783\n",
      " 0.437391 0.506364 0.544167 0.457391 0.375833 0.314348 0.423478 0.505\n",
      " 0.516667 0.187917 0.407826 0.605    0.577778 0.423043 0.697391 0.712174\n",
      " 0.68     0.876364 0.535    0.449583 0.318333 0.610417 0.789167 0.948261\n",
      " 0.551304 0.420833 0.       0.649565 0.594583 0.527391 0.496957 0.655652\n",
      " 0.776522 0.602917 0.525217 0.379167 0.47375  0.737391 0.624583 0.839565\n",
      " 0.495    0.394167 0.493913 0.302174 0.314167 0.646667 0.918333 0.68625\n",
      " 0.65375  0.48     0.42625  0.642083 0.470833 0.83625  0.8775   0.8575\n",
      " 0.716956 0.739167 0.819167 0.540417 0.67125  0.888333 0.479583 0.5425\n",
      " 0.665833 0.614167 0.407083 0.729583 0.887917 0.810833 0.776667 0.729167\n",
      " 0.835417 0.700833 0.503333 0.762083 0.73     0.697083 0.737083 0.444167\n",
      " 0.59     0.54125  0.631667 0.58875  0.489167 0.632917 0.7475   0.863333\n",
      " 0.9225   0.867083 0.787917 0.837917 0.87     0.829583 0.719583 0.626667\n",
      " 0.749583 0.81     0.740833 0.69625  0.6775   0.81875  0.685    0.636667\n",
      " 0.677083 0.305    0.354167 0.45625  0.6525   0.6      0.597917 0.622083\n",
      " 0.654583 0.747917 0.494583 0.507083 0.471667 0.688333 0.735833 0.670417\n",
      " 0.666667 0.74625  0.770417 0.7075   0.703333 0.573333 0.483333 0.513333\n",
      " 0.658333 0.634167 0.497917 0.39625  0.444583 0.6825   0.637917 0.590417\n",
      " 0.743333 0.65125  0.757917 0.609167 0.578333 0.635833 0.559167 0.47625\n",
      " 0.59125  0.585    0.604167 0.650417 0.707083 0.69125  0.580417 0.5\n",
      " 0.550833 0.757083 0.540833 0.402917 0.583333 0.465833 0.480833 0.49125\n",
      " 0.6575   0.7575   0.630833 0.755    0.752917 0.592083 0.570417 0.424167\n",
      " 0.42375  0.415    0.8175   0.712083 0.575417 0.722917 0.674167 0.77\n",
      " 0.47     0.455417 0.771667 0.76125  0.85     0.561765 0.554583 0.548333\n",
      " 0.639167 0.727083 0.716667 0.742083 0.790417 0.886957 0.917083 0.939565\n",
      " 0.897917 0.75375  0.71375  0.692174 0.7125   0.709167 0.718333 0.695\n",
      " 0.69     0.88125  0.9      0.902083 0.9725   0.845    0.848333 0.885417\n",
      " 0.84875  0.699167 0.6475   0.791667 0.760833 0.71     0.647917 0.620833\n",
      " 0.684167 0.70125  0.7275   0.73375  0.80875  0.90625  0.896667 0.71625\n",
      " 0.486667 0.579583 0.701667 0.895217 0.63625  0.574167 0.629167 0.74125\n",
      " 0.772083 0.622917 0.720417 0.812917 0.585833 0.8825   0.62375  0.68375\n",
      " 0.71875  0.702083 0.6225   0.519167 0.734583 0.75875  0.721667 0.758333\n",
      " 0.813333 0.44625  0.552917 0.458333 0.587083 0.68875  0.93     0.575833\n",
      " 0.41     0.502083 0.684583 0.91     0.9625   0.549167 0.64375  0.681667\n",
      " 0.698333 0.743043 0.830833 0.613333 0.524583 0.625833 0.612917 0.775833\n",
      " 0.827083 0.949583 0.970417 0.58     0.695833 0.5075   0.49     0.670833\n",
      " 0.66375  0.500417 0.560833 0.58625  0.6375   0.595417 0.858333 0.681304\n",
      " 0.506957 0.7625   0.503913 0.615833]\n",
      "[0.160446  0.248539  0.248309  0.160296  0.1869    0.0895652 0.168726\n",
      " 0.266804  0.36195   0.223267  0.122132  0.304627  0.301     0.126548\n",
      " 0.157963  0.188433  0.194017  0.146775  0.208317  0.195904  0.353242\n",
      " 0.17197   0.2466    0.15833   0.129796  0.29385   0.113837  0.1233\n",
      " 0.145365  0.0739826 0.187192  0.053213  0.264308  0.277752  0.127839\n",
      " 0.161079  0.1418    0.0454083 0.188839  0.221935  0.10855   0.203367\n",
      " 0.260883  0.417908  0.291374  0.251791  0.230104  0.264925  0.507463\n",
      " 0.223235  0.307846  0.195683  0.094113  0.250496  0.346539  0.186571\n",
      " 0.125248  0.289686  0.216425  0.307833  0.225754  0.203346  0.251871\n",
      " 0.343287  0.341352  0.12065   0.22015   0.261877  0.23297   0.220775\n",
      " 0.270604  0.136926  0.184309  0.203117  0.209579  0.231017  0.368167\n",
      " 0.207721  0.288783  0.22575   0.234261  0.243787  0.230725  0.209571\n",
      " 0.1843    0.212204  0.226996  0.172888  0.217646  0.258708  0.197146\n",
      " 0.182213  0.385571  0.388067  0.263063  0.162312  0.226992  0.133083\n",
      " 0.146767  0.324474  0.274879  0.250617  0.1107    0.226375  0.340808\n",
      " 0.303496  0.163567  0.157971  0.241925  0.325258  0.219521  0.192175\n",
      " 0.185333  0.3265    0.3122    0.320908  0.240063  0.235075  0.106354\n",
      " 0.183454  0.342667  0.328996  0.295392  0.228246  0.16045   0.0746375\n",
      " 0.176     0.115671  0.120642  0.189667  0.179725  0.13495   0.152979\n",
      " 0.126871  0.277354  0.201492  0.108213  0.125013  0.148008  0.233842\n",
      " 0.207092  0.154233  0.199642  0.240679  0.230092  0.213938  0.131225\n",
      " 0.111329  0.292287  0.253121  0.123142  0.138692  0.121896  0.187808\n",
      " 0.136817  0.149883  0.140554  0.15485   0.30535   0.269283  0.167912\n",
      " 0.206471  0.143029  0.119408  0.102     0.155475  0.171025  0.172262\n",
      " 0.238804  0.222025  0.0945333 0.107588  0.144283  0.261821  0.185312\n",
      " 0.102608  0.115062  0.228858  0.0814792 0.126258  0.1592    0.225129\n",
      " 0.183471  0.282337  0.200254  0.146133  0.240667  0.182833  0.208342\n",
      " 0.245033  0.215804  0.1306    0.113817  0.222021  0.1331    0.131221\n",
      " 0.169171  0.0908083 0.200258  0.183463  0.178479  0.174138  0.168537\n",
      " 0.164813  0.156717  0.20585   0.135583  0.19715   0.184696  0.22825\n",
      " 0.201487  0.151121  0.164796  0.125621  0.211454  0.222633  0.208954\n",
      " 0.236329  0.143667  0.233208  0.139308  0.104467  0.248754  0.27675\n",
      " 0.146763  0.253108  0.210833  0.0839625 0.375617  0.304659  0.159825\n",
      " 0.125008  0.0833333 0.141796  0.139929  0.185325  0.206467  0.212696\n",
      " 0.343943  0.0970208 0.192748  0.124379  0.153608  0.115054  0.088913\n",
      " 0.141804  0.1673    0.271146  0.164183  0.189675  0.178483  0.151742\n",
      " 0.134954  0.0964042 0.128125  0.0783667 0.0783833 0.0503792 0.118171\n",
      " 0.148629  0.172883  0.206475  0.292296  0.222013  0.0833458 0.205854\n",
      " 0.17725   0.0223917 0.0454042 0.06345   0.0423042 0.143042  0.24815\n",
      " 0.141787  0.223883  0.258083  0.281717  0.175379  0.110087  0.243339\n",
      " 0.422275  0.221396  0.0926667 0.0995125 0.118792  0.166658  0.148642\n",
      " 0.197763  0.229479  0.351371  0.176617  0.10635   0.135571  0.0820917\n",
      " 0.271779  0.189062  0.0920542 0.057225  0.0690375 0.0621958 0.189067\n",
      " 0.314675  0.212062  0.281721  0.306596  0.199633  0.136829  0.305362\n",
      " 0.168533  0.224496  0.18595   0.138054  0.335825  0.167304  0.0988958\n",
      " 0.0684208 0.142122  0.258092  0.271158  0.220158  0.100754  0.0957833\n",
      " 0.0839583 0.0622083 0.232583  0.266175  0.240058  0.0827167 0.233221\n",
      " 0.0665417 0.14055   0.0609583 0.268042  0.260575  0.243167  0.169779\n",
      " 0.172896  0.0615708 0.2214    0.047275  0.274246  0.190304  0.155091\n",
      " 0.239465  0.18845   0.293961  0.119412  0.134337  0.220154 ]\n",
      "[7 8 3 0 2 4 6 5 1 9]\n",
      "[1 0]\n",
      "[ 2  8 10  5  3  9  7  6  1  4]\n",
      "[2 1 3 0]\n",
      "[ 226  125   75   67   58   63   88   78   70   30   24   13   23   44\n",
      "  169  184   97    9   48   55   46   93  118   49  106    0   22   31\n",
      "  116  130   32   40   41   61   98  243   89   35   28   27  203  266\n",
      "  133  129  163  338  334  384  143   47   91   76  277  416   60  103\n",
      "  153   94  397  123  205  122   59  161  445  576  232  178  196  260\n",
      "  508  809  611  242  280  128  107  189  571  309  154  114  188  526\n",
      "  920  430  253  340  124  518  680  482  173  139  324  408  870  403\n",
      "  259  368  423  127  814  954  456  401  325  507 1082  660  355  159\n",
      "  357  509  914  400  414  407  545  891  458  388  316  425  524 1245\n",
      "  899  483  393  443  457  512 1115 1288  892  427  319  437  519 1048\n",
      "  944  411  451  418  358  489  980  841  491  420  448  500 1017  930\n",
      "  399  432  561 1007 1077  498  442  492  586  710 1228 1639  593  461\n",
      "  450  410 1111  987  433  446  730 1321 1112  496  455  385  375  628\n",
      "  652  365  452  379  424  906  888  478  300  585  864  763  503  535\n",
      "  481  854  767  429  406  465 1072  486  709  534  272  463  200  812\n",
      "  453  464 1080 1375  713  274  990  932  421  419  395  263  426  808\n",
      "  775  405  329  337  194  995  882  299  391  210  293  377  543 1224\n",
      " 1304  860  199  320  969  157  271  848  903  247   71  523  225  251\n",
      "  231  204  664  239  722  590  354  275   95  166  556  479  142   84\n",
      "  353  601  718  494   87  126  113  168  386  146   74  164  312  112\n",
      "  110  117  192  141  144  213  145  278]\n",
      "[ 654  670 1229 1454 1518 1362  891  768 1280 1220 1137 1368 1367 1026\n",
      "  953  883  674 1572 1844 1468  888  836 1330 1799  472  416 1129  975\n",
      "  956 1459 1313 1489 1620  905 1269 1592 1466 1552 1491 1597 1184 1192\n",
      " 1705 1675 1897 2216 2348 1103 1173  912 1376 1778 1707 1341 1545 1708\n",
      " 1365 1714 1903 1562 1730 1437  491 1628 1817 1700  577 1408 1435 1687\n",
      " 1767 1871 2320 2355 1693 1424 1676 2243 1918 1699 1910 1515 1221 1806\n",
      " 2108 1506 1920 1354 1598 2381 2395 2570 1299 1576 2493 1777 1953 2738\n",
      " 2484 2186 2760 2795 3331 3444 2574 2481 3300 3722 3325 3489 3717 3347\n",
      " 2213 3554 3848 2378 3819 3714 3102 2932 3698 4109 3632 4169 3413 2507\n",
      " 2971 3185 3445 3319 3840 4008 3547 3084 3438 3833 4238 3919 3808 2757\n",
      " 2433 2549 3309 3461 4232 4414 3473 3221 3875 4070 3725 3352 3771 3237\n",
      " 2993 4157 4164 4411 3222 3981 3312 3105 3311 4061 3846 4044 4022 3420\n",
      " 3385 3854 3916 4377 4488 4116 2915 2367 2978 3634 3845 3838 3348 3138\n",
      " 3363 3596 3594 4196 4220 3505 3296 3617 3789 3688 3152 2825 2298 2556\n",
      " 3272 3901 3784 3176 2916 2778 3537 3107 3777 3843 2773 2487 3480 3695\n",
      " 3896 3980 2646 2482 3563 4004 4026 3166 3356 3277 2624 3925 4614 4181\n",
      " 3893  889 2919 3905 4429 4370 4332 3852 2419 2115 2506 1878 1689 3127\n",
      " 3595 4023 4062 4138 3231 4018 3077 2921 3203 3813 4240 2137 3647 3466\n",
      " 3946 3643 3427 4186 4372 1949 2302 3240 3970 4267 4126 4036 3174 3114\n",
      " 3603 2199 2623 3115 3318 3293 3857 4111 2170 3724 3628 2809 2762 3488\n",
      " 3992 3490 3291  570 2446 3307 3658 3816 3656 3576 2770 2697 3662 3829\n",
      " 3804 2743 2928 2792 2713 3891 3746 1672 2914 3147 2720 2733 2545 1538\n",
      " 2454  935 1697 1819 2261 3614 2818 3425 3545 3672 2908 2851 3578 2468\n",
      "  655 3172 3359 2688 2366 3167 3368 3562 3528 3399 2464 2211 3143 3534\n",
      " 2553 2841 2046  856  451  887 1059 2047 2169 2508 1820]\n",
      "[ 880  795 1304 1521 1576 1581 1450  969  838 1310 1244 1150 1391 1411\n",
      " 1195 1137  980  683 1620 1899 1514  981  954 1379 1905  472  438 1160\n",
      " 1091 1086 1490 1345 1494 1530 1681 1003 1512 1501 1580 1518 1694 1387\n",
      " 1458 1838 1763 2026 2379 2686 1437 1557 1055 1423 1869 1768 1417 1822\n",
      " 2124 1425 1817 2056 1656 1873 1834  614 1781 2022  636 1891 1853 2011\n",
      " 1919 1945 2067 2580 2863 2502 2035 1918 2523 2046 1806 2099 2086 1960\n",
      " 2311 1482 1604 2108 1880 2518 2811 1744 2648 2910 2094 2387 2975 1950\n",
      " 2092 3062 2892  798 3056 3163 3054 3699 3867 1633 3388 3435 3756 4123\n",
      " 3650 3829 4224 4429 2873 4036 4203 2537 4176 4223 4016 3741 4098 4523\n",
      " 3970 4576 3816 3052 3862 3643 3833 3635 4265 4532 4792 3983 3921 4226\n",
      " 4681 4376 4320 3872 3721 3441 3736 3780 4669 4933 4521 4165 4286 4143\n",
      " 3710 4260 4217 3834 4648 4584 4859 3547 4481 4329 4035 3718 4506 4245\n",
      " 4476 4583 4427 4462 4352 4358 4869 5074 4826 3612 4617 4227 4306 4288\n",
      " 3758 4459 4125 3796 4003 4040 4704 4950 4408 4113 4244 4088 3537 3200\n",
      " 2926 3208 3637 4292 4163 3600 3822 3666 4522 3407 4233 4428 3250 4230\n",
      " 4414 4461 4465 3500 3249 4013 4433 4432 3550 3821 4349 3354 4411 5323\n",
      " 4715 3379 4356 1089 3731 4338 4882 4781 4796 4360 3629 3794 2828 2635\n",
      " 1981 1811 3401 4585 4345 4444 4533 3494 3885 3696 4253 3475 4142 4577\n",
      " 2331 4642 4348 3943 3726 4853 2265 2681 3450 4263 4604 4503 4579 4398\n",
      " 4418 4463 4284 2338 2822 4366 4262 4277 4486 2327 3995 4019 3657 3665\n",
      " 3895 4393 3737 3557  641 2969 3532 3909 4047 3860 3853 3434 3258 3904\n",
      " 4068 2871 3221 3514 3303 4021 1770 3009 3313 3276 3212 2687 1596 2538\n",
      " 1288 2298 2755 3798 2905 3551 3658 3840 3331 3237 3724 2542  677 3263\n",
      " 3523 3000 2619 3262 3466 3674 3638 3516 2656 2380 3297 3675 2627 2984\n",
      " 2156 1000  664 1158 1126 2191 2314 2786 2225]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(data[i].unique())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nkDe68NkyZ2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          365\n",
       "season                        4\n",
       "year                          1\n",
       "holiday                       2\n",
       "weekday                       7\n",
       "workingday                    2\n",
       "weather                       3\n",
       "temp                        294\n",
       "feel_temp                   353\n",
       "hum                         332\n",
       "windspeed                   349\n",
       "promotion_level              10\n",
       "promotion_type                2\n",
       "promotion_level_external     10\n",
       "promotion_type_external       4\n",
       "casual                      288\n",
       "registered                  347\n",
       "cnt                         345\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the value counts (frequency of occurence) of the unique values in these features ['season', 'year', 'weather', 'promotion_type']\n",
    "data.apply(pd.Series.nunique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIsIIQWFmJRK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xXsOHMilNTz"
   },
   "outputs": [],
   "source": [
    "# drop the feature 'id' as it has no information to deliver.\n",
    "\n",
    "data = data.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4BXZ-_qmNYh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1G-eQ8ULmO3J"
   },
   "outputs": [],
   "source": [
    "# one hot encode the categorical columns.\n",
    "data=pd.get_dummies(data, columns=[\"season\",\"weather\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcf6heimmZaQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 22)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of data \n",
    "# notice the increase in the no. of features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLOkfanFfWDK"
   },
   "source": [
    "Notice that our target feature \"cnt\" is the sum of the features \"registered\" + \"casual\"<br>\n",
    "\n",
    "To avoid data leakage remove the feature \"casual\" for the training purpose. <br>\n",
    "\n",
    "To understand more about data leakage refer the article mentioned in the uselful links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZ2HK7RGmO63"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into X and y\n",
    "# While loading data into X drop the columns \"cnt\" and \"casual\". \n",
    "X = ?\n",
    "\n",
    "# notice the target variable is 'cnt'\n",
    "y = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1kT6PpR4F1s"
   },
   "outputs": [],
   "source": [
    "# store the names of the training features / name of the columns used for training. [Very important step for visualization later.]\n",
    "\n",
    "train_columns = list(X.columns)\n",
    "print(train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6blYZrz7mPBG"
   },
   "outputs": [],
   "source": [
    "# Apply scaling if our data is spread across wide differences of range values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Feyrb90p3qf8"
   },
   "outputs": [],
   "source": [
    "# print the type of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jtau8HV73t7Y"
   },
   "source": [
    "Note : <br>\n",
    "Type of X should be pandas dataframe.\n",
    "If not then convert X into pandas DataFrame object before proceeding further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5II0AWy4AHK"
   },
   "outputs": [],
   "source": [
    "# convert X into pandas Dataframe\n",
    "# in the parameters specify columns = train_columns.\n",
    "\n",
    "X = pd.DataFrame(X, columns = train_columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ad4MfGe1mPDk"
   },
   "outputs": [],
   "source": [
    "# split the dataset into X_train, X_test, y_train, y_test\n",
    "# play around with test sizes.\n",
    "\n",
    "test_size = ?\n",
    "X_train, X_test, y_train, y_test = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6czlxW2tnQqP"
   },
   "outputs": [],
   "source": [
    "# print the shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-lp9O74nUm7"
   },
   "outputs": [],
   "source": [
    "# build the Linear Regression model.\n",
    "\n",
    "model = ?\n",
    "\n",
    "# fit the model on the training data\n",
    "model.fit(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga_f-66JoVri"
   },
   "outputs": [],
   "source": [
    "# print the score on training set\n",
    "y_pred_train = model.predict(?)\n",
    "print(\"On Training set : \", r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKmlrtpDoilu"
   },
   "outputs": [],
   "source": [
    "# print the score on the test set\n",
    "y_pred_test = model.predict(?)\n",
    "print(\"On testing set : \", r2_score(?, ?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYyTP91_pB-u"
   },
   "source": [
    "Do not edit the code given below. Observe the distribution of weights. \n",
    "Which feature has the maximum coefficient ? <br>\n",
    "Keep this figure as a base reference for visualizing the effects of l1-norm and l2-norm later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ck8Vx1bWozcf"
   },
   "outputs": [],
   "source": [
    "# custom summary function to plot the coefficients / weightage of the features.\n",
    "def custom_summary(model, column_names, title):\n",
    "    '''Show a summary of the trained linear regression model'''\n",
    "\n",
    "    # Plot the coeffients as bars\n",
    "    fig = plt.figure(figsize=(8,len(column_names)/3))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    rects = plt.barh(column_names, model.coef_,color=\"lightblue\")\n",
    "\n",
    "    # Annotate the bars with the coefficient values\n",
    "    for rect in rects:\n",
    "        width = round(rect.get_width(),4)\n",
    "        plt.gca().annotate('  {}  '.format(width),\n",
    "                    xy=(0, rect.get_y()),\n",
    "                    xytext=(0,2),  \n",
    "                    textcoords=\"offset points\",  \n",
    "                    ha='left' if width<0 else 'right', va='bottom')        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuyEBDrYotzb"
   },
   "outputs": [],
   "source": [
    "# coefficients plot\n",
    "# let's call the above custom function.\n",
    "\n",
    "custom_summary(model, train_columns, \"Linear Regression coefficients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E77vWyd7plQ2"
   },
   "outputs": [],
   "source": [
    "# evaluate the model with k = 10 Fold Cross validation\n",
    "\n",
    "folds = KFold(n_splits = 10, shuffle = True, random_state = 100)\n",
    "results = cross_val_score(?, ?, ?, scoring = 'r2', cv = folds)\n",
    "\n",
    "print(type(model).__name__)\n",
    "print(\"kFoldCV:\")\n",
    "print(\"Fold R2 scores:\", results)\n",
    "print(\"Mean R2 score:\", results.mean())\n",
    "print(\"Std R2 score:\", results.std())\n",
    "print(\"Generalizability on training set : \", results.mean(), \" +/- \", results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNVstf3MRWTS"
   },
   "source": [
    "Feature Selection using Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fWmra-LRSvk"
   },
   "outputs": [],
   "source": [
    "print(\"Original shape of X_train : \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdfMwVybRl9o"
   },
   "outputs": [],
   "source": [
    "# check the variance of X.\n",
    "# Note the type(X) should be a pandas DataFrame as stated earlier.\n",
    "\n",
    "X.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kk36hVk_Rv6l"
   },
   "source": [
    "Remove low variance features using Variance Threshold. \n",
    "\n",
    "Note : If the variance is less, it implies the values of that particular feature spans limited range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RkbbmcuRtkJ"
   },
   "outputs": [],
   "source": [
    "# play around with the threshold values\n",
    "\n",
    "sel = VarianceThreshold(threshold = (0.01))\n",
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bk525DOySWJp"
   },
   "outputs": [],
   "source": [
    "# do not edit.\n",
    "\n",
    "selected_features = list(X_train.columns[sel.get_support()])\n",
    "print(\"Selected features : \", selected_features)\n",
    "print(\"Removed features : \", list(X_train.columns[~sel.get_support()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci3BFPnE5ex4"
   },
   "outputs": [],
   "source": [
    "# Delete the removed features from the train_columns list.\n",
    "\n",
    "for i in removed_features:\n",
    "  train_columns.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1AQhlAhSXI5"
   },
   "outputs": [],
   "source": [
    "#transform / remove the low variance features\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH8mZBs3S-i3"
   },
   "source": [
    "## Lasso Regression : L1 - norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XTVvk4gS98-"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(?, ?, test_size = ?, random_state = 100)\n",
    "\n",
    "# hyperparamater alpha : controls the degree of penaliation.\n",
    "# play around with alpha values.\n",
    "alpha = 1.0\n",
    "\n",
    "#create the model\n",
    "model_lasso = Lasso(alpha = alpha)\n",
    "\n",
    "#fit the model on training data\n",
    "model_lasso.fit(?, ?)\n",
    "\n",
    "#calculate the score on training data\n",
    "y_pred_train = model_lasso.predict(?)\n",
    "print(\"On train set : \", r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PD4NpuxYTWoV"
   },
   "outputs": [],
   "source": [
    "#evaluate the model on testing data\n",
    "y_pred_test = model_lasso.predict(?)\n",
    "print(\"On test set : \", r2_score(y_test, ?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bN7w1V5hT2Dp"
   },
   "outputs": [],
   "source": [
    "# visualize the coefficients.\n",
    "# compare the results with the plot obtained earlier.\n",
    "\n",
    "custom_summary(model_lasso, train_columns, \"Lasso Regression Coefficients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsjziyZzUS8P"
   },
   "source": [
    "We can see that Lasso regression has automatically done a lot of feature selection. Some columns might have zero coefficients. It has been effectively removed. <br> \n",
    "The model is much more interpretable than the baseline linear regression model.\n",
    "<br>\n",
    "Hence, Lasso regression has embedded Feature Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9IlW2V2UfD0"
   },
   "source": [
    "# Ridge Regression : L2 - norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6PRlLONUckx"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# hyperparamater alpha : controls the degree of penaliation.\n",
    "# play around with alpha values.\n",
    "alpha = 1.0\n",
    "\n",
    "#create the model\n",
    "model_ridge = Ridge(alpha = ?)\n",
    "\n",
    "#fit the model on training data\n",
    "model_ridge.fit(?, ?)\n",
    "\n",
    "#calculate the score on training data\n",
    "y_pred_train = model_ridge.predict(?)\n",
    "print(\"On train set : \", r2_score(y_train, ?))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzukav_PVTG2"
   },
   "outputs": [],
   "source": [
    "#evaluate the model on testing data\n",
    "y_pred_test = model_ridge.predict(?)\n",
    "print(\"On test set : \", r2_score(y_test, ?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c39ZBnmvVYSA"
   },
   "outputs": [],
   "source": [
    "# visualize the coefficients.\n",
    "# compare the results with the plot obtained earlier.\n",
    "\n",
    "custom_summary(model_ridge, train_columns, \"Ridge Regression Coefficients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqLF3812VefE"
   },
   "source": [
    "Ridge regression doesn't drive smaller coefficients to 0 hence it doesn't possess internal feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUCm0xOc6inT"
   },
   "source": [
    "Points to Ponder ! [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9FHk5sw6nLb"
   },
   "source": [
    "Did you notice the highest dependency on the feature \"registered\" if you haven't removed it till now ?\n",
    "\n",
    "Since our target is \"cnt\" which is the simple combination of \"registered\" and \"casual\".\n",
    "\n",
    "we have removed \"casual\", but the model was smart enough to predict the target \"cnt\" simply from one feature \"registered\" itself. \n",
    "\n",
    "This is the classic example of Data Leakage. So the aim here is not to make 99 percent accurate predictions, the aim is to take into account the factors for making predictions.\n",
    "\n",
    "So, to get a detailed report, we should avoid data leakage thereby removing both the features \"registered\" and \"casual\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4euMcAFR6kLG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1_LinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
