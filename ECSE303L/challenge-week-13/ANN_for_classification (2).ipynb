{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3FobpTFCnjg"
   },
   "source": [
    "# Wheat varieties prediction using ANN\n",
    "## Objective: To implement artificial neural network from scratch\n",
    "\n",
    "\n",
    "\n",
    "## Artificial neural network\n",
    "\n",
    "Artificial neural networks are relatively crude electronic networks of neurons based on the neural structure of the brain. They process records one at a time, and learn by comparing their classification of the record (i.e., largely arbitrary) with the known actual classification of the record. The errors from the initial classification of the first record is fed back into the network, and used to modify the networks algorithm for further iterations.\n",
    "\n",
    "A neuron in an artificial neural network is\n",
    "\n",
    "1. A set of input values (xi) and associated weights (wi).\n",
    "\n",
    "2. A function (g) that sums the weights and maps the results to an output (y).\n",
    "\n",
    "### Parameters\n",
    "We can choose the dimensionality (the number of nodes) of the hidden layer. The more nodes we put into the hidden layer the more complex functions we will be able fit. But higher dimensionality comes at a cost. First, more computation is required to make predictions and learn the network parameters. A bigger number of parameters also means we become more prone to overfitting our data.\n",
    "How to choose the size of the hidden layer? While there are some general guidelines and recommendations, it always depends on your specific problem and is more of an art than a science. We will play with the number of nodes in the hidden layer later on and see how it affects our output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlhZ_25gDmTR"
   },
   "source": [
    "###Activation function\n",
    "We also need to pick an *activation function* for our hidden layer. The activation function transforms the inputs of the layer into its outputs. A nonlinear activation function is what allows us to fit nonlinear hypotheses. Common chocies for activation functions are [tanh](https://reference.wolfram.com/language/ref/Tanh.html), the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), or [ReLUs](https://en.wikipedia.org/wiki/Rectifier_(neural_networks). \n",
    "\n",
    "\n",
    "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Remember that our goal is to find the parameters that minimize our loss function. We can use [gradient descent](http://cs231n.github.io/optimization-1/) to find its minimum. I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these, and ideally you would also [decay the learning rate over time](http://cs231n.github.io/neural-networks-3/#anneal).\n",
    "\n",
    "### Gradient descent\n",
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output. I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here](http://cs231n.github.io/optimization-2/)) floating around the web.\n",
    "\n",
    "### Backpropagation formula\n",
    "Applying the backpropagation formula we find the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_3 = \\hat{y} - y \\\\\n",
    "& \\delta_2 = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWJYEzB_Dh8e"
   },
   "source": [
    "## Dataset\n",
    "Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.\n",
    "\n",
    "## Attributes\n",
    "1.\tarea A,\n",
    "2.\tperimeter P,\n",
    "3.\tcompactness C = 4*pi*A/P^2,\n",
    "4.\tlength of kernel,\n",
    "5.\twidth of kernel,\n",
    "6.\tasymmetry coefficient\n",
    "7.\tlength of kernel groove.\n",
    "\n",
    "## Target Class\n",
    "Varieties of wheat: Kama, Rosa and Canadian.\n",
    "\n",
    "## Source: https://www.kaggle.com/rwzhang/seeds-dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN_gXZRbDbUO"
   },
   "source": [
    "## Task 1: Implement Artificial Neural Networks from Scratch\n",
    "1.\tObtain the seeds dataset\n",
    "2.\tApply pre-processing techniques (if any)\n",
    "3.\tDivide dataset into training (70%) and testing (30%) set, respectively.\n",
    "4.\tImplement Artificial Neural Networks from Scratch\n",
    "5.\tTrain your neural model from scratch\n",
    "6.\tEvaluate the training and testing accuracy of your own model\n",
    "\n",
    "## Task 2: Implement SKLEARN’s Artificial Neural Networks\n",
    "1.\tObtain the seeds dataset\n",
    "2.\tApply pre-processing techniques (if any)\n",
    "3.\tDivide dataset into training (70%) and testing (30%) set, respectively.\n",
    "4.\tImplement Artificial Neural Networks using SKLEARN’s library\n",
    "5.\tTrain SKLEARN’s neural model \n",
    "6.\tEvaluate the training and testing accuracy of SKLEARN’s model\n",
    "\n",
    "# Task 3: Play with hyper-parameters\n",
    "1.\tUse SKLEARN’s neural model\n",
    "2.\tEvaluate the impact of various hyper-parameters of neural networks\n",
    "3.\tPlot the results\n",
    "4.\tConclude the experiments by showing the impact of hyper-parameters on neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9iUOR3hDc-0"
   },
   "source": [
    "###Helpful links: \n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf\n",
    "https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-JpzeOsD6jg"
   },
   "source": [
    "# Additonal work (not considered in evaluation)\n",
    "Instead of batch gradient descent, use minibatch gradient descent ([more info](http://cs231n.github.io/optimization-1/#gd))\n",
    "\n",
    "Use various activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9aMK8ze6bUY"
   },
   "source": [
    "##Task 1: Implement Artificial Neural Networks from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ti-tdq2M600q"
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import preprocessing, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GHFWQz1R6-8x"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15.26</th>\n",
       "      <th>14.84</th>\n",
       "      <th>0.871</th>\n",
       "      <th>5.763</th>\n",
       "      <th>3.312</th>\n",
       "      <th>2.221</th>\n",
       "      <th>5.22</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.462</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.49</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>5.563</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.586</td>\n",
       "      <td>5.219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>5.420</td>\n",
       "      <td>3.302</td>\n",
       "      <td>2.700</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.63</td>\n",
       "      <td>15.46</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>6.053</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.040</td>\n",
       "      <td>5.877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.969</td>\n",
       "      <td>5.533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.242</td>\n",
       "      <td>4.543</td>\n",
       "      <td>5.314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   15.26  14.84   0.871  5.763  3.312  2.221   5.22  1\n",
       "0  14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "1  14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "2  13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "3  16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
       "4  14.38  14.21  0.8951  5.386  3.312  2.462  4.956  1\n",
       "5  14.69  14.49  0.8799  5.563  3.259  3.586  5.219  1\n",
       "6  14.11  14.10  0.8911  5.420  3.302  2.700  5.000  1\n",
       "7  16.63  15.46  0.8747  6.053  3.465  2.040  5.877  1\n",
       "8  16.44  15.25  0.8880  5.884  3.505  1.969  5.533  1\n",
       "9  15.26  14.85  0.8696  5.714  3.242  4.543  5.314  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset \n",
    "#data = pd.read_csv('seeds_dataset.txt', sep=\" \", header=None)\n",
    "data = pd.read_csv('seeds_dataset.txt', delimiter = \"\\t\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxEfVyAE7ASj"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Encoding categorical variables (if any)\n",
    "# Feature Scaling\n",
    "# Filling missing values (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9YAXm8H7C9L"
   },
   "outputs": [],
   "source": [
    "# Divide the dataset to training (70 %) and testing set (30 %)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_KayhYF75UK"
   },
   "outputs": [],
   "source": [
    "# Define initial parameters of ANN and gradient descent:\n",
    "# input layer dimensionality, output layer dimensionality, learning rate (epsilon) for gradient descent and regularization strength (lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "necwumMA8nfX"
   },
   "outputs": [],
   "source": [
    "# Define total loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUKcHH_M8r92"
   },
   "outputs": [],
   "source": [
    "# Prediction function using activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_kkRSp083xe"
   },
   "outputs": [],
   "source": [
    "# Write downn ANN code from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9Y8QmmB87ES"
   },
   "outputs": [],
   "source": [
    "# Train your own ANN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exW8ef4l9Hr_"
   },
   "outputs": [],
   "source": [
    "# Test your own ANN model and evaluate performance in terms of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJDNjH1c6g6b"
   },
   "source": [
    "##Task 2: Implement SKLEARN’s Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuA3ZOMjtCR8"
   },
   "outputs": [],
   "source": [
    "#Obtain the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-9wh8xW9Ok3"
   },
   "outputs": [],
   "source": [
    "# Build SKLEARN's ANN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9ksVY5l9eyZ"
   },
   "outputs": [],
   "source": [
    "# Test your own ANN model and evaluate performance in terms of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x7jKz446syl"
   },
   "source": [
    "# Task 3: Play with hyper-parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaDXM9dM6unb"
   },
   "source": [
    "1.\tUse SKLEARN’s neural model\n",
    "2.\tEvaluate the impact of various hyper-parameters of neural networks\n",
    "3.\tPlot the results\n",
    "4.\tConclude the experiments by showing the impact of hyper-parameters on neural network"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ANN_for_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
